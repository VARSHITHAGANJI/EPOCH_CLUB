\documentclass[12pt,letterpaper, onecolumn]{exam}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{sidecap}
\usepackage{tabularx}
\usepackage{csquotes}
\usepackage{makecell}
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=black,
    pdftitle={Overleaf Example},
    pdfpagemode=FullScreen,
}
%\usepackage[left=0.5cm,right=0.5cm,top=0.5cm,bottom=0.5cm]{geometry}
\usepackage[usestackEOL]{stackengine}
%\setstacktabbedgap{1ex} 
\usepackage{tikz}
\usetikzlibrary{decorations.pathreplacing}
\usetikzlibrary{fadings}
\def\layersep{2.5cm}

\usepackage{enumitem}
\usepackage{algorithm}
\usepackage{algpseudocode}

%\usepackage[shortlabels]{enumitem}
%\usepackage{enumerate}
\usepackage[lmargin=71pt, tmargin=0.8in]{geometry}  %For centering solution box

% \chead{\hline} % Un-comment to draw line below header
\thispagestyle{empty}   %For removing header/footer from page 1

\usepackage{listings}
\usepackage{xcolor}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}

\lstset{style=mystyle}




\begin{document}



\newtheorem{theorem}{Theorem}[section]
\newtheorem{problem}{Problem}
\newtheorem{proposition}{Proposition}[section]
\newtheorem{lemma}{Lemma}[section]
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{example}{Example}[section]
\newtheorem{definition}[problem]{Definition}

\newcommand{\BEQA}{\begin{eqnarray}}
\newcommand{\EEQA}{\end{eqnarray}}
\newcommand{\define}{\stackrel{\triangle}{=}}
\bibliographystyle{IEEEtran}
\raggedbottom
\setlength{\parindent}{0pt}
\providecommand{\mbf}{\mathbf}
\providecommand{\norm}[1]{\lVert#1\rVert}
\providecommand{\pr}[1]{\ensuremath{\Pr\left(#1\right)}}
\providecommand{\qfunc}[1]{\ensuremath{Q\left(#1\right)}}
\providecommand{\sbrak}[1]{\ensuremath{{}\left[#1\right]}}
\providecommand{\lsbrak}[1]{\ensuremath{{}\left[#1\right.}}
\providecommand{\rsbrak}[1]{\ensuremath{{}\left.#1\right]}}
\providecommand{\brak}[1]{\ensuremath{\left(#1\right)}}
\providecommand{\lbrak}[1]{\ensuremath{\left(#1\right.}}
\providecommand{\rbrak}[1]{\ensuremath{\left.#1\right)}}
\providecommand{\cbrak}[1]{\ensuremath{\left\{#1\right\}}}
\providecommand{\lcbrak}[1]{\ensuremath{\left\{#1\right.}}
\providecommand{\rcbrak}[1]{\ensuremath{\left.#1\right\}}}
\let\vec\mathbf

\newlist{mydesc}{description}{1} % create a new list called mydesc, of type "description"
\setlist[mydesc]{
  align=left, % use the align-format defined above
  leftmargin=0pt, % indentation for all the lines
  labelindent=1em, % horizontal space before label
  labelsep=0pt
   % horizontal space after label -- set to zero because we add space via "leftwithbar"
}



\begingroup  
    \centering
    
    \LARGE Weekly Report 2 - KNN\\[0.5em]
    
    \large Ganji Varshitha\par
    \large AI20BTECH11009\par
\endgroup
\rule{\textwidth}{0.4pt}
\pointsdroppedatright   %Self-explanatory
\printanswers
\newcommand\Solution{
  \textbf{Solution:}\\}
\newcommand{\myvec}[1]{\ensuremath{\begin{bmatrix}#1\end{bmatrix}}}
 %Replace "Ans:" with starting keyword in solution box

 \subsection*{Introduction}
KNN algorithm, also known as Lazy learning is a non parametric model which is used to solve classification and regression problems. 



\subsection*{Algorithm}
It follows the instance based learning as the learning part involves storing all the data points and performs the action at the time of classification or regression. Choosing the metric and value of K is very crucial in this model.
\begin{algorithm}
\caption{KNN Algorithm}\label{cap}
\begin{algorithmic}

\State Calculate distance of new point from all the training data points.
\State Sort the distances in increasing order with corresponding train data and select first K data points.
\If{Classification}
\State Class is determined by the majority class of the selected K data points.
\EndIf
\If{Regression}
\State The predicted continuous target value is given by mean or median value of the K data points.
\EndIf
\end{algorithmic}
\end{algorithm}




\subsection*{How to determine K ?}
Without loss of generality, K needs to be odd number to avoid ties in
binary classification.\\
For small values of K, the model has high variance and overfits the data. For example K = 1, the model is highly sensitive to outliers.
Training error is low whereas test error is high.\\
For high values of K, model has less variance and increased bias.\\
We can determine optimal value of K by iterating through various values of K and choosing the one with minimum error rate.

\subsection*{Distance metrics}
Euclidean distance is the most common metric used. Besides, there are Manhattan, Minkowski and hamming distance metrics used when needed.



\subsection*{Key points}
\begin{itemize}
\item Since calculating distances involve all the features, the scale of the features may affect the outcome as one feature may dominate the rest. Therefore, we need to normalize the features.
\item The computational cost is very high as it involves calculating distances from all the training data points.
\item Training is fast whereas classification task is time consuming.
\item After finding the optimal value of K, the model is robust to outliers.
\end{itemize}

\subsection*{Questions}

\begin{questions}
\question[] Does KNN work for large dataset?
\begin{choices}
\choice Yes
\choice No
\end{choices}
\begin{Solution}
B. No
\end{Solution}
\question[] State Manhattan distance formula.\\
\begin{Solution}
\begin{align*}
D_m(x,y) = {}& \sum_{i=1}^n |x_i-y_i|
\end{align*}
\end{Solution}
\question[] What can be done to overcome curse of dimensionality?\\
\begin{Solution}
As number of dimensions increases, size of data space increases. This reduces the density of data. It can be avoided by reducing the number of features by dimensionality reduction.
\end{Solution}
\question[] Hamming distance is useful for \rule{2cm}{0.15mm}
\begin{choices}
\choice Numerical features
\choice Categorical features
\end{choices}
\begin{Solution}
Since hamming distance tells number of indifferent features, it is helpful in categorical input. 
\end{Solution}
\question[] Time complexity of the algorithm is \rule{2cm}{0.15mm}\\
\begin{Solution}
Assume K cluster for N data points having D dimensions.\\
Time complexity is:
$\mathcal{O}(\#\text{iter}*K*N*D)$
\end{Solution}
\end{questions}



\end{document}