\documentclass[12pt,letterpaper, onecolumn]{exam}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{sidecap}
\usepackage{tabularx}
\usepackage{csquotes}
\usepackage{makecell}
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=black,
    pdftitle={Overleaf Example},
    pdfpagemode=FullScreen,
}
%\usepackage[left=0.5cm,right=0.5cm,top=0.5cm,bottom=0.5cm]{geometry}
\usepackage[usestackEOL]{stackengine}
%\setstacktabbedgap{1ex} 
\usepackage{tikz}
\usetikzlibrary{decorations.pathreplacing}
\usetikzlibrary{fadings}
\def\layersep{2.5cm}

\usepackage{enumitem}
\usepackage{algorithm}
\usepackage{algpseudocode}

%\usepackage[shortlabels]{enumitem}
%\usepackage{enumerate}
\usepackage[lmargin=71pt, tmargin=0.8in]{geometry}  %For centering solution box

% \chead{\hline} % Un-comment to draw line below header
\thispagestyle{empty}   %For removing header/footer from page 1

\usepackage{listings}
\usepackage{xcolor}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}

\lstset{style=mystyle}




\begin{document}



\newtheorem{theorem}{Theorem}[section]
\newtheorem{problem}{Problem}
\newtheorem{proposition}{Proposition}[section]
\newtheorem{lemma}{Lemma}[section]
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{example}{Example}[section]
\newtheorem{definition}[problem]{Definition}

\newcommand{\BEQA}{\begin{eqnarray}}
\newcommand{\EEQA}{\end{eqnarray}}
\newcommand{\define}{\stackrel{\triangle}{=}}
\bibliographystyle{IEEEtran}
\raggedbottom
\setlength{\parindent}{0pt}
\providecommand{\mbf}{\mathbf}
\providecommand{\norm}[1]{\lVert#1\rVert}
\providecommand{\pr}[1]{\ensuremath{\Pr\left(#1\right)}}
\providecommand{\qfunc}[1]{\ensuremath{Q\left(#1\right)}}
\providecommand{\sbrak}[1]{\ensuremath{{}\left[#1\right]}}
\providecommand{\lsbrak}[1]{\ensuremath{{}\left[#1\right.}}
\providecommand{\rsbrak}[1]{\ensuremath{{}\left.#1\right]}}
\providecommand{\brak}[1]{\ensuremath{\left(#1\right)}}
\providecommand{\lbrak}[1]{\ensuremath{\left(#1\right.}}
\providecommand{\rbrak}[1]{\ensuremath{\left.#1\right)}}
\providecommand{\cbrak}[1]{\ensuremath{\left\{#1\right\}}}
\providecommand{\lcbrak}[1]{\ensuremath{\left\{#1\right.}}
\providecommand{\rcbrak}[1]{\ensuremath{\left.#1\right\}}}
\let\vec\mathbf

\newlist{mydesc}{description}{1} % create a new list called mydesc, of type "description"
\setlist[mydesc]{
  align=left, % use the align-format defined above
  leftmargin=0pt, % indentation for all the lines
  labelindent=1em, % horizontal space before label
  labelsep=0pt
   % horizontal space after label -- set to zero because we add space via "leftwithbar"
}



\begingroup  
    \centering
    
    \LARGE Weekly Report 2 - KNN\\[0.5em]
    
    \large Ganji Varshitha\par
    \large AI20BTECH11009\par
\endgroup
\rule{\textwidth}{0.4pt}
\pointsdroppedatright   %Self-explanatory
\printanswers
\newcommand\Solution{
  \textbf{Solution:}\\}
\newcommand{\myvec}[1]{\ensuremath{\begin{bmatrix}#1\end{bmatrix}}}
 %Replace "Ans:" with starting keyword in solution box

 \subsection*{Introduction}
KNN algorithm, also known as Lazy learning is a non parametric model which is used to solve classification and regression problems. 



\subsection*{Algorithm}
It follows the instance based learning as the learning part involves storing all the data points and performs the action at the time of classification or regression. Choosing the metric and value of K is very crucial in this model.
\begin{algorithm}
\caption{KNN Algorithm}\label{cap}
\begin{algorithmic}

\State Calculate distance of new point from all the training data points.
\State Sort the distances in increasing order with corresponding train data and select first K data points.
\If{Classification}
\State Class is determined by the majority class of the selected K data points.
\EndIf
\If{Regression}
\State The predicted continuous target value is given by mean or median value of the K data points.
\EndIf
\end{algorithmic}
\end{algorithm}




\subsection*{How to determine K ?}
Without loss of generality, K needs to be odd number to avoid ties in
binary classification.\\
For small values of K, the model has high variance and overfits the data. For example K = 1, the model is highly sensitive to outliers.
Training error is low whereas test error is high.\\
For high values of K, model has less variance and increased bias.\\
We can determine optimal value of K by iterating through various values of K and choosing the one with minimum error rate.

\subsection*{Distance metrics}
Euclidean distance is the most common metric used. Besides, there are Manhattan, Minkowski and hamming distance metrics used when needed.





Decision trees are prone to overfit which results in high variance of the model. Bagging reduces the variance of the model.\\
Let S be the training dataset. \\
Let ${S_k}$ be a sequence of training sets containing a sub-set of S.\\
 Let P be the underlying distribution of S.\\
Bagging replaces the prediction of the model with the majority of 
the predictions given by the classifiers S.
\begin{align}
\phi(x,P) = {}& \mathbb{E}_s(\phi(x,S_k)))
\end{align}


\begin{itemize}
\item The algorithm works if the dependent variables are linearly related with independent variables.
\item It assumes the error terms to have constant variance and no correlation between one another.
\item Since we need to compute the gradients, there is need to normalize the training data so that it does not take longer time.
\item Learning rate decides how fast the algorithm converges.
\item The model tends to overfit if there are many features in the dataset. Besides, there is a high chance correlation between some input variables.
\end{itemize}

\subsection*{Questions}

\begin{questions}
\question[] Bias and variance in linear regression model are \rule{2cm}{0.15mm} related.\\
\begin{oneparchoices}
    \choice directly
    \choice inversely
  \end{oneparchoices} \\
  \begin{Solution}
  B. inversely
  \end{Solution}
  \question[] Explain the geometric and probabilistic interpretation of the model.\\
  \begin{Solution}
  Geometric Interpretation:\\
  The least-squares regression function is obtained by finding the orthogonal projection of the output vector y onto the subspace spanned by ${x_1,x_2,x_3,\cdots,x_d }$.\\
  Probabilistic Interpretation:\\
  Let us assume the target variable be given by the deterministic function with added gaussian noise. This gives
  \begin{align}
  p(y|X, \vec{w}, \beta) = \prod_{n=1}^N \mathcal{N}(y_n|\vec{w}^\top x_n,\beta^{-1})
  \end{align}
  where $\beta$ is inverse variance of zero mean Gaussian random variable. We estimate the probability model using maximum likelihood which is same as minimising the least squares error.
  \begin{align}
  \arg \max_{\vec{w}} L = \arg \min_{\vec{w}} E
  \end{align}
  \end{Solution}
  \question[] Do the missing values in the data affect the model?\\
  \begin{oneparchoices}
    \choice Yes
    \choice No
  \end{oneparchoices} \\
  \begin{Solution}
  B. No
  \end{Solution}
  \question[] Is the model generative or discriminative? Explain why.
  \\
  \begin{Solution}
  Linear regression is a discriminative model. As seen in the probabilistic interpretation of the model above, we learn the parameters that maximises the conditional probability P$(Y|X)$. This is the basic definition of discriminative model where we assume functional form of P$(Y|X)$ and estimate its parameters. 
  \end{Solution}
  \question[] When to prefer gradient descent approach to ordinary least squares method?\\
  \begin{Solution}
  Time complexity of least squares method is $\mathcal{O}(n^3)$ whereas time complexity of gradient descent is $\mathcal{O}(n)$.
  Gradient descent is preferred to ordinary least squares method when n i.e number of input features is greater than 10,000.
  
  \end{Solution}
  \question[] Which error function is sensitive to outliers?
  \begin{choices}
  \choice MSE(Mean squared error)
  \choice MAE(Mean absolute error)
  \end{choices}
  \begin{Solution}
  A. MSE
  \end{Solution}
\end{questions}









\end{document}