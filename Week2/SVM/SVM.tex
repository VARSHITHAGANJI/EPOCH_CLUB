\documentclass[12pt,letterpaper, onecolumn]{exam}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{sidecap}
\usepackage{tabularx}
\usepackage{csquotes}
\usepackage{makecell}
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=black,
    pdftitle={Overleaf Example},
    pdfpagemode=FullScreen,
}
%\usepackage[left=0.5cm,right=0.5cm,top=0.5cm,bottom=0.5cm]{geometry}
\usepackage[usestackEOL]{stackengine}
%\setstacktabbedgap{1ex} 
\usepackage{tikz}
\usetikzlibrary{decorations.pathreplacing}
\usetikzlibrary{fadings}
\def\layersep{2.5cm}

\usepackage{enumitem}
\usepackage{algorithm}
\usepackage{algpseudocode}

%\usepackage[shortlabels]{enumitem}
%\usepackage{enumerate}
\usepackage[lmargin=71pt, tmargin=0.8in]{geometry}  %For centering solution box

% \chead{\hline} % Un-comment to draw line below header
\thispagestyle{empty}   %For removing header/footer from page 1

\usepackage{listings}
\usepackage{xcolor}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}

\lstset{style=mystyle}




\begin{document}



\newtheorem{theorem}{Theorem}[section]
\newtheorem{problem}{Problem}
\newtheorem{proposition}{Proposition}[section]
\newtheorem{lemma}{Lemma}[section]
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{example}{Example}[section]
\newtheorem{definition}[problem]{Definition}

\newcommand{\BEQA}{\begin{eqnarray}}
\newcommand{\EEQA}{\end{eqnarray}}
\newcommand{\define}{\stackrel{\triangle}{=}}
\bibliographystyle{IEEEtran}
\raggedbottom
\setlength{\parindent}{0pt}
\providecommand{\mbf}{\mathbf}
\providecommand{\norm}[1]{\lVert#1\rVert}
\providecommand{\pr}[1]{\ensuremath{\Pr\left(#1\right)}}
\providecommand{\qfunc}[1]{\ensuremath{Q\left(#1\right)}}
\providecommand{\sbrak}[1]{\ensuremath{{}\left[#1\right]}}
\providecommand{\lsbrak}[1]{\ensuremath{{}\left[#1\right.}}
\providecommand{\rsbrak}[1]{\ensuremath{{}\left.#1\right]}}
\providecommand{\brak}[1]{\ensuremath{\left(#1\right)}}
\providecommand{\lbrak}[1]{\ensuremath{\left(#1\right.}}
\providecommand{\rbrak}[1]{\ensuremath{\left.#1\right)}}
\providecommand{\cbrak}[1]{\ensuremath{\left\{#1\right\}}}
\providecommand{\lcbrak}[1]{\ensuremath{\left\{#1\right.}}
\providecommand{\rcbrak}[1]{\ensuremath{\left.#1\right\}}}
\let\vec\mathbf

\newlist{mydesc}{description}{1} % create a new list called mydesc, of type "description"
\setlist[mydesc]{
  align=left, % use the align-format defined above
  leftmargin=0pt, % indentation for all the lines
  labelindent=1em, % horizontal space before label
  labelsep=0pt
   % horizontal space after label -- set to zero because we add space via "leftwithbar"
}



\begingroup  
    \centering
    
    \LARGE Weekly Report 2 - SVM\\[0.5em]
    
    \large Ganji Varshitha\par
    \large AI20BTECH11009\par
\endgroup
\rule{\textwidth}{0.4pt}
\pointsdroppedatright   %Self-explanatory
\printanswers
\newcommand\Solution{
  \textbf{Solution:}\\}
\newcommand{\myvec}[1]{\ensuremath{\begin{bmatrix}#1\end{bmatrix}}}
 %Replace "Ans:" with starting keyword in solution box

 \subsection*{Introduction}
SVM is a supervised learning model used for both classification and regression. It creates decision boundary which divides the training data into classes, hence making it a discriminative classifier.
It is non parametric as it does not take any information from the training data.

\subsection*{Algorithm}
The algorithm creates a decision boundary in n-dimensional plane so as to classify the new points.\\ 
There are two types of SVM: linear SVM and non-linear SVM.
\subsubsection*{Linear SVM}
Goal is to create a straight which separates the points of 2 classes. There are infinite lines which can be drawn. \\
\textbf{Margin} of the classifier is the width that the boundary could be increased by before hitting a data point.\\
In order to avoid misclassification we need to maximise the margin of the classifier, i.e we draw a line between the 2 closest points that keeps the other data points separated.\\
Therefore, the data points that the margin pushes up against are called \textbf{Support Vectors}.\\
We can formulate the maximum margin problem as quadratic optimization problem.
\begin{equation}
\begin{aligned}
\min_{\vec{w},b} & \hspace{4mm} \frac{1}{2}\vec{w}^\top \vec{w} \\
\textrm{s.t.   } & y_{i}(\vec{w}^\top \vec{x_i}+b)\geq 1
\end{aligned}
\end{equation}
After computing optimal $\vec{w}$ and b, the classifier is given by
sign($\vec{w}^\top \vec{x}+b$). We assumed that all the positive points have $\vec{w}^\top \vec{x}+b > 1$ and negative points have 
$\vec{w}^\top \vec{x}+b < 1$ in the training dataset.\\
From the knowledge of Primal and duality, we can say that strong duality holds. The Lagrangian of the problem is 
\begin{equation}
\begin{aligned}
L(\vec{w},b,\vec{\alpha}) = {}& \frac{1}{2}||\vec{w}||^2 + \sum_{i = 1}^n \vec{\alpha}_i(1-y_i(\vec{w}^\top \vec{x}_i+b))
\end{aligned}
\end{equation}
Since optimal solution for primal is same as that of dual, solving the dual with KKT conditions, we have 
\begin{equation}
\begin{aligned}
\max_{\vec{\alpha}\geq 0} \hspace{2mm} \min_{\vec{w},b} &\frac{1}{2}||\vec{w}||^2 + \sum_{i = 1}^n \vec{\alpha_i}(1-y_i(\vec{w}^\top \vec{x_i}+b))\\
\frac{\partial L}{\partial \vec{w}} ={}& 0 \implies  \vec{w} = \sum_{j} \vec{\alpha}_j y_j \vec{x}_j\\
\frac{\partial L}{\partial b} ={}& 0 \implies \sum_{j} \vec{\alpha}_j y_j=0
\end{aligned}
\end{equation}
Substituting values of $\vec{w},b$ in the dual, we get
\begin{equation}
\begin{aligned}
\max & \sum_{k=1}^R \alpha_k - \frac{1}{2}\sum_{k=1}^R \sum_{l=1}^R \vec{\alpha}_k \vec{\alpha}_l Q_{kl} \text{ where } Q_{kl} = y_k y_l(x_k \cdot x_l)\\ 
\textrm{ s.t    }& \vec{\alpha}_k \geq 0 \; \; \forall \; k \; \; \; \sum_{k=1}^R \vec{\alpha}_k y_k = 0
\end{aligned}
\end{equation}
Solving the maximisation problem, we get optimal values of $\vec{w},b$ as
\begin{align}
\vec{w} = {}& \frac{1}{2} \sum_{k=1}^R \vec{\alpha}_k y_k \vec{x}_k\\
b = {}& -y_i(y_i(x_i \cdot w)-1)
\end{align}
\subsubsection*{Non Linear SVM}
The best hyperplane is found by using the basis functions i.e kernel functions which classifies in the transformed high dimensional feature space.\\ The dot product in eq (4) is replaced by kernel functions.\\
Common kernels include polynomial and gaussian kernels.\\
Polynomial kernel: $k(\vec{x}_i \cdot \vec{x}_j) = (\vec{x}_i \cdot \vec{x}_j)^d$\\
Gaussian kernel: $k(\vec{x}_i \cdot \vec{x}_j) = \exp \big( -\frac{(\vec{x}_i - \vec{x}_j)^2}{2 \sigma^2} \big)$\\

\subsection*{Questions}
\begin{questions}
\question[] Is the entire training set involved in predicting?\\
\begin{Solution}
No, only support vectors are used. No other points affect the decision boundary.
\end{Solution}

\question[] Which data points have non zero alpha weights in the dual problem?
\begin{choices}
\choice All the training data points
\choice Support vectors
\end{choices}

\begin{Solution}
B. Support vectors.
\end{Solution}

\question[] Does SVM ouptut a conditional probability of class?\\
\begin{Solution}
No.
\end{Solution}

\question[] Explain the methods to deal with multi class problem.\\
\begin{Solution}
There are two approaches: One vs One and One vs All.\\
One vs One : For n classes we will have n(n-1)/2 svm's for every pair of classes
\\ One vs All: We have n svm's where datapoints belonging to one class are selected against rest of the data points.
\end{Solution}

\question[] Does changing the normalization constant in linear classifier affect the model?\\
\begin{Solution}
No.
\end{Solution}
\end{questions}





\end{document}