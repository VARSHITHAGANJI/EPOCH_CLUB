\documentclass[12pt,letterpaper, onecolumn]{exam}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{sidecap}
\usepackage{tabularx}
\usepackage{csquotes}
\usepackage{makecell}
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=black,
    pdftitle={Overleaf Example},
    pdfpagemode=FullScreen,
}
%\usepackage[left=0.5cm,right=0.5cm,top=0.5cm,bottom=0.5cm]{geometry}
\usepackage[usestackEOL]{stackengine}
%\setstacktabbedgap{1ex} 
\usepackage{tikz}
\usetikzlibrary{decorations.pathreplacing}
\usetikzlibrary{fadings}
\def\layersep{2.5cm}

\usepackage{enumitem}

%\usepackage[shortlabels]{enumitem}
%\usepackage{enumerate}
\usepackage[lmargin=71pt, tmargin=0.8in]{geometry}  %For centering solution box

% \chead{\hline} % Un-comment to draw line below header
\thispagestyle{empty}   %For removing header/footer from page 1

\usepackage{listings}
\usepackage{xcolor}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}

\lstset{style=mystyle}




\begin{document}



\newtheorem{theorem}{Theorem}[section]
\newtheorem{problem}{Problem}
\newtheorem{proposition}{Proposition}[section]
\newtheorem{lemma}{Lemma}[section]
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{example}{Example}[section]
\newtheorem{definition}[problem]{Definition}

\newcommand{\BEQA}{\begin{eqnarray}}
\newcommand{\EEQA}{\end{eqnarray}}
\newcommand{\define}{\stackrel{\triangle}{=}}
\bibliographystyle{IEEEtran}
\raggedbottom
\setlength{\parindent}{0pt}
\providecommand{\mbf}{\mathbf}
\providecommand{\norm}[1]{\lVert#1\rVert}
\providecommand{\pr}[1]{\ensuremath{\Pr\left(#1\right)}}
\providecommand{\qfunc}[1]{\ensuremath{Q\left(#1\right)}}
\providecommand{\sbrak}[1]{\ensuremath{{}\left[#1\right]}}
\providecommand{\lsbrak}[1]{\ensuremath{{}\left[#1\right.}}
\providecommand{\rsbrak}[1]{\ensuremath{{}\left.#1\right]}}
\providecommand{\brak}[1]{\ensuremath{\left(#1\right)}}
\providecommand{\lbrak}[1]{\ensuremath{\left(#1\right.}}
\providecommand{\rbrak}[1]{\ensuremath{\left.#1\right)}}
\providecommand{\cbrak}[1]{\ensuremath{\left\{#1\right\}}}
\providecommand{\lcbrak}[1]{\ensuremath{\left\{#1\right.}}
\providecommand{\rcbrak}[1]{\ensuremath{\left.#1\right\}}}
\let\vec\mathbf

\newlist{mydesc}{description}{1} % create a new list called mydesc, of type "description"
\setlist[mydesc]{
  align=left, % use the align-format defined above
  leftmargin=0pt, % indentation for all the lines
  labelindent=1em, % horizontal space before label
  labelsep=0pt
   % horizontal space after label -- set to zero because we add space via "leftwithbar"
}



\begingroup  
    \centering
    
    \LARGE Weekly Report 2- Naive Bayes\\[0.5em]
    
    \large Ganji Varshitha\par
    \large AI20BTECH11009\par
\endgroup
\rule{\textwidth}{0.4pt}
\pointsdroppedatright   %Self-explanatory
\printanswers
\newcommand\Solution{
  \textbf{Solution:}\\}
\newcommand{\myvec}[1]{\ensuremath{\begin{bmatrix}#1\end{bmatrix}}}
 %Replace "Ans:" with starting keyword in solution box

 \subsection*{Introduction}
Naive Bayes is a supervised learning algorithm which predicts the most probable class. It is very useful in text classification. Some of the applications include spam filtration, sentiment analysis, etc.

\subsection*{Why the name Naive Bayes?}
It is called Bayes because it estimates the bayesian probability of a class.\\
Let us assume a data point x has n features $A_1, A_2,\cdots A_n$. The posterior probability $P(C|A_1, A_2,\cdots A_n)$ where C denotes class is given by bayes theorem:
\begin{align}
P(C|A_1, A_2,\cdots A_n) = {}& \frac{P(A_1, A_2,\cdots A_n |C) P(C)}{P(A_1, A_2,\cdots A_n)}
\end{align}
It is naive because we assume independence of features $A_i$ when class is given.\\
This gives likelihood $P(A_1, A_2,\cdots A_n |C) = P(A_1|C) P(A_2|C) \cdots P(A_n|C)$.

\subsection*{Algorithm}
From the above theorem, we know that we should choose a class which maximises posterior probability i.e $P(C|A_1, A_2,\cdots A_n)$. \\
Since $P(A_1, A_2,\cdots A_n)$ is same for all values of C, we can omit the marginal probability.
\begin{align}
\hat{C} = \arg \max_{C} (P(A_1|C) P(A_2|C) \cdots P(A_n|C))P(C)
\end{align}
This is called Maximum A Posteriori estimation.


\subsection*{Key points}
\begin{itemize}
\item The algorithm works if the dependent variables are linearly related with independent variables.
\item It assumes the error terms to have constant variance and no correlation between one another.
\item Since we need to compute the gradients, there is need to normalize the training data so that it does not take longer time.
\item Learning rate decides how fast the algorithm converges.
\item The model tends to overfit if there are many features in the dataset. Besides, there is a high chance correlation between some input variables.
\end{itemize}

\subsection*{Questions}

\begin{questions}
\question[] Bias and variance in linear regression model are \rule{2cm}{0.15mm} related.\\
\begin{oneparchoices}
    \choice directly
    \choice inversely
  \end{oneparchoices} \\
  \begin{Solution}
  B. inversely
  \end{Solution}
  \question[] Explain the geometric and probabilistic interpretation of the model.\\
  \begin{Solution}
  Geometric Interpretation:\\
  The least-squares regression function is obtained by finding the orthogonal projection of the output vector y onto the subspace spanned by ${x_1,x_2,x_3,\cdots,x_d }$.\\
  Probabilistic Interpretation:\\
  Let us assume the target variable be given by the deterministic function with added gaussian noise. This gives
  \begin{align}
  p(y|X, \vec{w}, \beta) = \prod_{n=1}^N \mathcal{N}(y_n|\vec{w}^\top x_n,\beta^{-1})
  \end{align}
  where $\beta$ is inverse variance of zero mean Gaussian random variable. We estimate the probability model using maximum likelihood which is same as minimising the least squares error.
  \begin{align}
  \arg \max_{\vec{w}} L = \arg \min_{\vec{w}} E
  \end{align}
  \end{Solution}
  \question[] Do the missing values in the data affect the model?\\
  \begin{oneparchoices}
    \choice Yes
    \choice No
  \end{oneparchoices} \\
  \begin{Solution}
  B. No
  \end{Solution}
  \question[] Is the model generative or discriminative? Explain why.
  \\
  \begin{Solution}
  Linear regression is a discriminative model. As seen in the probabilistic interpretation of the model above, we learn the parameters that maximises the conditional probability P$(Y|X)$. This is the basic definition of discriminative model where we assume functional form of P$(Y|X)$ and estimate its parameters. 
  \end{Solution}
  \question[] When to prefer gradient descent approach to ordinary least squares method?\\
  \begin{Solution}
  Time complexity of least squares method is $\mathcal{O}(n^3)$ whereas time complexity of gradient descent is $\mathcal{O}(n)$.
  Gradient descent is preferred to ordinary least squares method when n i.e number of input features is greater than 10,000.
  
  \end{Solution}
  \question[] Which error function is sensitive to outliers?
  \begin{choices}
  \choice MSE(Mean squared error)
  \choice MAE(Mean absolute error)
  \end{choices}
  \begin{Solution}
  A. MSE
  \end{Solution}
\end{questions}









\end{document}